<!DOCTYPE html>
<head>
    <title>CS663 Research Tutorial</title>
    <link rel="stylesheet" href="index.css" />
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {},
            },
        };
    </script>
</head>
<body class="dark:bg-slate-800">
    <nav>
        <ul
            class="text-2xl flex flex-row justify-evenly slate-300 dark:slate-500 text-black dark:text-white"
        >
            <li>
                <a href="/"><div class="p-4">Abstract</div></a>
            </li>
            <li>
                <a href="/introduction"><div class="p-4">Introduction</div></a>
            </li>
            <li>
                <a href="/sensors"><div class="p-4">Sensors</div></a>
            </li>
            <li>
                <a href="/models"><div class="p-4">Models</div></a>
            </li>
            <li>
                <a href="/tradeoffs"><div class="p-4">Tradeoffs</div></a>
            </li>
            <li>
                <a href="/challenges">
                    <div class="p-4">Challenges</div>
                </a>
            </li>
            <li>
                <a href="/conclusions"><div class="p-4">Conclusions</div> </a>
            </li>
        </ul>
    </nav>
    <div class="p-8 w-full min-h-screen">
        <h1 class="mb-4 text-4xl text-center font-semibold dark:text-white">
            Conclusions
        </h1>
        <p class="text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; A system to provide depth information can be a useful asset
            to improve the capabilities of a visually-impaired person beyond
            traditional technologies. Furthermore, as models and hardware
            advance, the quality of depth information and efficiency of running
            inference will improve. Novel approaches continue to build on top of
            the original CNNs and Vision Transformers, incorporating new types
            of sensors as well. Furthermore, as application development and user
            experience research advances, there may be superior ways of
            developing such applications and conveying depth information.
        </p>
        <h2 class="p-2 text-3xl text-center dark:text-slate-200">
            Device Hardware
        </h2>
        <p class="text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; As the hardware in mobile devices and servers improves and as
            more specialized hardware is developed, the capabilities of depth
            estimation models and applications will improve. While currently,
            smartphones do not have massive parallel processing as graphics
            chips do, future mobile devices might include more chips optimized
            for running machine learning models. Furthermore, general
            improvements in CPU, battery, networking, and memory would enable
            developers to pursue more complex methods that can produce better
            results. The manufacturing process for the hardware may also
            improve, resulting in reduced costs for all of the components.
        </p>
        <h2 class="p-2 text-3xl text-center dark:text-slate-200">Sensors</h2>
        <p class="text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; Though advanced sensors such as LiDAR are not as widely
            available as monocular cameras on smartphones, new sensors could be
            incorporated as mobile devices improve. It is possible that stereo
            vision may become more commonplace and models could incorporate the
            sensor into their input. Furthemore, as monocular cameras themselves
            acquire more color detail and capture higher-definition images,
            models may have more data to work with when inferring depth.
        </p>
    </div>
</body>

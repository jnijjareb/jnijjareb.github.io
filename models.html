<!DOCTYPE html>
<head>
    <title>CS663 Research Tutorial</title>
    <link rel="stylesheet" href="index.css" />
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {},
            },
        };
    </script>
    `
</head>
<body class="dark:bg-slate-800">
    <nav>
        <ul
            class="text-2xl flex flex-row justify-evenly slate-300 dark:slate-500 text-black dark:text-white"
        >
            <li>
                <a href="/"><div class="p-4">Abstract</div></a>
            </li>
            <li>
                <a href="/introduction"><div class="p-4">Introduction</div></a>
            </li>
            <li>
                <a href="/sensors"><div class="p-4">Sensors</div></a>
            </li>
            <li>
                <a href="/models"><div class="p-4">Models</div></a>
            </li>
            <li>
                <a href="/tradeoffs"><div class="p-4">Tradeoffs</div></a>
            </li>
            <li>
                <a href="/challenges">
                    <div class="p-4">Challenges</div>
                </a>
            </li>
            <li>
                <a href="/conclusions"><div class="p-4">Conclusions</div> </a>
            </li>
        </ul>
    </nav>
    <div class="p-8 w-full min-h-screen">
        <h1 class="mb-4 text-4xl text-center font-semibold dark:text-white">
            Models
        </h1>
        <p class="text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; A depth estimation model&apos;s inputs will depend on the
            combination of sensors used. Typically, a 2D image will be
            normalized and fed into the model. For LiDAR, a sparse point cloud
            is included. The output of the depth estimation model provides a
            relative distance of each pixel from the perspective the image is
            taken from. For such data, the most established approach is to use a
            Convolutional Neural Network (CNN). Convolutions are well-suited for
            2-dimensional data and can be parallelized for high performance.
            Vision Transformers are another established method providing
            attention between visual patches of the image, which can provide an
            advantage over CNNs. Hybrid approaches between Vision Transformers
            and CNNs also can perform well.
        </p>
        <h2 class="p-2 text-3xl text-center dark:text-slate-200">
            Convolutional Neural Networks
        </h2>
        <div
            class="mx-auto w-2/3 flex flex-row items-center justify-items-center font-light text-lg dark:text-white"
        >
            <div class="p-4 w-1/2">
                <p class="my-auto">
                    &emsp; Given how widely applicable CNNs are to vision tasks,
                    CNNs are a great baseline to begin for depth estimation
                    tasks. CNNs for image tasks rely on 2-dimensional
                    convolutions to extract features from the image. While
                    traditionally, particular filters such as those for edge
                    detection or blur are used, CNNs use the Backpropagation
                    Algorithm in order to learn the weights of the convolution
                    kernel. The kernel is slid over the image, accumulating the
                    pixel data covered by the kernel using a matrix dot product.
                    The features are then flattened and passed into a standard
                    feed-forward neural network.
                </p>
            </div>
            <div class="p-4 w-1/2">
                <figure>
                    <img
                        class="p-2 max-w-1/2 w-3/4 h-auto rounded-xl bg-slate-400"
                        src="https://mandroid6.github.io/images/Convolution_schematic.gif"
                        alt="Visualization of Convolution"
                    />
                </figure>
                <figcaption class="">
                    <a
                        class=""
                        href="https://mandroid6.github.io"
                        target="_blank"
                    >
                        <p
                            class="mx-auto p-1 text-blue-800 dark:text-blue-200 font-semibold text-center"
                        >
                            Visualization of 2D Convolution with 3x3 Kernel
                        </p>
                    </a>
                </figcaption>
            </div>
        </div>

        <h2 class="p-2 text-3xl text-center dark:text-slate-200">
            Vision Transformers
        </h2>
        <div
            class="mx-auto w-2/3 flex flex-row items-center justify-items-center font-light text-lg dark:text-white"
        >
            <div class="p-4 w-1/2">
                <figure>
                    <img
                        class="p-2 max-w-1/2 w-3/4 h-auto rounded-xl bg-slate-400"
                        src="assets/transformer.png"
                        alt="Transformer Architecture"
                    />
                </figure>
                <figcaption class="">
                    <a
                        class=""
                        href="https://arxiv.org/pdf/2204.05007v1"
                        target="_blank"
                    >
                        <p
                            class="mx-auto p-1 text-blue-800 dark:text-blue-200 font-semibold text-center"
                        >
                            Visualization of Transformer Architecture in HiMODE
                        </p>
                    </a>
                </figcaption>
            </div>
            <div class="p-4 w-1/2">
                <p class="my-auto">
                    &emsp; On the other hand, Vision Transformers (ViT) are a
                    promising alternative to CNNs. Utilizing the attention
                    mechanism from the original Transformer architecture, ViTs
                    compute attention between patches of an image to better
                    correlate the relationships between different parts of an
                    image. With such a message passing mechanism, depth
                    estimation models can get a better sense fo the overall
                    context as opposed.to only having local feature information
                    as is the case with convolution kernels. Of course, given
                    that we are expecting the application to run on mobile,
                    computing requirements have to be considered, as CNNs tend
                    to be more efficient.
                </p>
            </div>
        </div>

        <h2 class="p-2 text-3xl text-center dark:text-slate-200">HiMODE</h2>
        <p class="mt-2 text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; In addition to CNNs and ViTs, there are hybrid approaches
            that combine the two. One particular example is HiMODE, an
            omnidirectional monocular depth estimation model [<a
                class="text-blue-400"
                href="introduction/#ref3"
                >3</a
            >]. While the sensor is different due to being omnidirectional, the
            concept for the model architecure still applies. HiMODE models the
            Transformer architecture but incorporates a convolution feature
            extractor and a Spatial Residual Block (SRB) which includes
            convolutions. The SRB itself is comprised of 3 sub-blocks, the
            results of which are concatenated to acquire the output of the
            entire block. The subblocks include a convolution, a zero-padded
            average pool, and a linear layer. The SRBs are placed between the
            encoder and decoder portions of the whole architecture and after the
            decoder.
        </p>

        <div class="mt-2 text-lg font-light w-1/2 mx-auto dark:text-white">
            <figure>
                <img
                    class="p-2 w-full h-auto rounded-xl bg-slate-400"
                    src="assets/himode.png"
                    alt="HiMODE Architecture"
                />
            </figure>
            <figcaption class="">
                <a
                    class=""
                    href="https://arxiv.org/pdf/2204.05007v1"
                    target="_blank"
                >
                    <p
                        class="mx-auto p-1 text-blue-800 dark:text-blue-200 font-semibold text-center"
                    >
                        HiMODE Architecture
                    </p>
                </a>
            </figcaption>
        </div>
        <h2 class="p-2 text-3xl text-center dark:text-slate-200">METER</h2>
        <p class="mt-2 text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; Though generally ViTs are preferred for heavier tasks, there
            exists a particular Transformer-like hybrid architecture optimized
            for mobile devices. METER is an encoder-decoder Transformer
            architecture that includes both MobileNet convolutional blocks and a
            few Transformer blocks, creating an overall lightweight model
            architecture well-suited to mobile and embedded devices [<a
                class="text-blue-400"
                href="introduction/#ref4"
                >4</a
            >]. METER also includes its own characteristic blocks which are part
            of the model and has skip connections from the encoder portion to
            recapture lost details from the encoder component of the
            Transformer. The METER blocks begin by passing the input into a
            convolutional block and a transformer block, the output of which
            gets concatenated with the original input again. That value is then
            passed into another convolutional block. Notably, the input features
            of the METER block are also the skip connections passed to the
            upsampling blocks that occur later in the architecture. Overall,
            METER may provide performance even better than that of traditional
            CNNs let alone ViTs and is an excellent choice for depth estimation.
        </p>

        <div class="mt-2 text-lg font-light w-1/2 mx-auto dark:text-white">
            <figure>
                <img
                    class="p-2 w-full h-auto rounded-xl bg-slate-400"
                    src="https://arxiv.org/html/2403.08368v1/x2.png"
                    alt="METER Architecture"
                />
            </figure>
            <figcaption class="">
                <a
                    class=""
                    href="https://arxiv.org/html/2403.08368v1"
                    target="_blank"
                >
                    <p
                        class="mx-auto p-1 text-blue-800 dark:text-blue-200 font-semibold text-center"
                    >
                        METER Architecture
                    </p>
                </a>
            </figcaption>
        </div>
        <h2 class="p-2 text-3xl text-center dark:text-slate-200">
            HybridDepth
        </h2>
        <p class="mt-2 text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; Given there are several CNN, ViT, and hybrid options for
            relative depth estimation, there also exist methods for metric depth
            estimation. One particular example, HybridDepth, is an architecture
            designed for mobile devices. It utilizes a technique known as Depth
            From Focus (DFF) in conjunction with standard relative depth
            estimation methods discussed prior. DFF is a traditional method that
            "estimates depth by identifying the focus distance at which each
            pixel is most sharply defined" [<a
                class="text-blue-400"
                href="introduction/#ref5"
                >5</a
            >]. While the focus is on monocular depth estimation, HybridDepth
            demonstrates that monocular cameras performing relative depth
            estimation can serve as the foundation for more complex processes
            such as metric depth estimation. In the HybridDepth architecture,
            the Least-Squares Fitting method is used to combine the relative
            depth map and the DFF results and calculate the scale and shift of
            the relative depth map and generate a Globally Scaled Depth Map. A
            larger scale would indicate a greater metric difference between the
            least and greatest values in the depth map while the shift
            identifies the metric depth of the least depth pixel. The Globally
            Scaled Depth Map is then combined channel-wise with the results from
            DFF to generate another scale map. Finally, the resulting scale map
            is passed into the refinement layer, whose output is multiplied with
            the original Globally Scaled Depth Map. The refinement layer itself
            focuses on harnessing the DFF values to "apply scale refinement to
            each pixel of the globally scaled depth map", resulting in superior
            performance [<a class="text-blue-400" href="introduction/#ref5">5</a
            >].
        </p>
        <div class="mt-2 text-lg font-light w-1/2 mx-auto dark:text-white">
            <figure>
                <img
                    class="p-2 w-full h-auto rounded-xl bg-slate-400"
                    src="assets/hybriddepth.png"
                    alt="HybridDepth Architecture"
                />
            </figure>
            <figcaption class="">
                <a
                    class=""
                    href="https://arxiv.org/pdf/2407.18443v1"
                    target="_blank"
                >
                    <p
                        class="mx-auto p-1 text-blue-800 dark:text-blue-200 font-semibold text-center"
                    >
                        HybridDepth Architecture
                    </p>
                </a>
            </figcaption>
        </div>
    </div>
</body>

<!doctype html>
<head>
    <title>CS663 Research Tutorial</title>
    <link rel="stylesheet" href="index.css" />
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {},
            },
        };
    </script>
    `
</head>
<body class="dark:bg-slate-800">
    <nav>
        <ul
            class="text-2xl flex flex-row justify-evenly slate-300 dark:slate-500 text-black dark:text-white"
        >
            <li>
                <a href="/"><div class="p-4">Abstract</div></a>
            </li>
            <li>
                <a href="/introduction"><div class="p-4">Introduction</div></a>
            </li>
            <li>
                <a href="/sensors"><div class="p-4">Sensors</div></a>
            </li>
            <li>
                <a href="/models"><div class="p-4">Models</div></a>
            </li>
            <li>
                <a href="/tradeoffs"><div class="p-4">Tradeoffs</div></a>
            </li>
            <li>
                <a href="/challenges">
                    <div class="p-4">Challenges</div>
                </a>
            </li>
            <li>
                <a href="/conclusions"><div class="p-4">Conclusions</div> </a>
            </li>
        </ul>
    </nav>
    <div class="p-8 w-full min-h-screen">
        <h1 class="mb-4 text-4xl text-center font-semibold dark:text-white">
            Models
        </h1>
        <p class="text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; A depth estimation model&apos;s inputs will depend on the
            combination of sensors used. Typically, a 2D image will be
            normalized and fed into the model. For LiDAR, a sparse point cloud
            is included. The output of the depth estimation model provides a
            relative distance of each pixel from the perspective the image is
            taken from. For such data, the most established approach is to use a
            Convolutional Neural Network (CNN). Convolutions are well-suited for
            2-dimensional data and can be parallelized for high performance.
            Vision Transformers are another established method providing
            attention between visual patches of the image, which can provide an
            advantage over CNNs. Hybrid approaches between Vision Transformers
            and CNNs also can perform well.
        </p>
        <h1>Depth Maps</h1>
        <div
            class="mx-auto w-2/3 flex flex-row items-center justify-items-center font-light text-lg dark:text-white"
        >
            <div class="p-4 w-1/2">
                <p class="my-auto">
                    &emsp; Given how widely applicable CNNs are to vision tasks,
                    CNNs are a great baseline to begin for depth estimation
                    tasks. CNNs for image tasks rely on 2-dimensional
                    convolutions to extract features from the image. While
                    traditionally, particular filters such as those for edge
                    detection or blur are used, CNNs use the Backpropagation
                    Algorithm in order to learn the weights of the convolution
                    kernel. The kernel is slid over the image, accumulating the
                    pixel data covered by the kernel using a matrix dot product.
                    The features are then flattened and passed into a standard
                    feed-forward neural network.
                </p>
            </div>
            <div class="p-4 w-1/2">
                <figure>
                    <img
                        class="p-2 max-w-1/2 w-3/4 h-auto rounded-xl bg-slate-400"
                        src="https://mandroid6.github.io/images/Convolution_schematic.gif"
                        alt="Visualization of Convolution"
                    />
                </figure>
                <figcaption class="">
                    <a
                        class=""
                        href="https://mandroid6.github.io"
                        target="_blank"
                    >
                        <p
                            class="mx-auto p-1 text-blue-800 dark:text-blue-200 font-semibold text-center"
                        >
                            Visualization of 2D Convolution with 3x3 Kernel
                        </p>
                    </a>
                </figcaption>
            </div>
        </div>

        <div
            class="mx-auto w-2/3 flex flex-row items-center justify-items-center font-light text-lg dark:text-white"
        >
            <div class="p-4 w-1/2">
                <figure>
                    <img
                        class="p-2 max-w-1/2 w-3/4 h-auto rounded-xl bg-slate-400"
                        src="/transformer.png"
                        alt="Transformer Architecture"
                    />
                </figure>
                <figcaption class="">
                    <a
                        class=""
                        href="https://arxiv.org/pdf/2204.05007v1"
                        target="_blank"
                    >
                        <p
                            class="mx-auto p-1 text-blue-800 dark:text-blue-200 font-semibold text-center"
                        >
                            Visualization of Transformer Architecture in HiMODE
                        </p>
                    </a>
                </figcaption>
            </div>
            <div class="p-4 w-1/2">
                <p class="my-auto">
                    &emsp; On the other hand, Vision Transformers (ViT) are a
                    promising alternative to CNNs. Utilizing the attention
                    mechanism from the original Transformer architecture, ViTs
                    compute attention between patches of an image to better
                    correlate the relationships between different parts of an
                    image. With such a message passing mechanism, depth
                    estimation models can get a better sense fo the overall
                    context as opposed.to only having local feature information
                    as is the case with convolution kernels. Of course, given
                    that we are expecting the application to run on mobile,
                    computing requiremnts have to be considered, as CNNs tend to
                    be more efficient.
                </p>
            </div>
        </div>
        <p class="mt-2 text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; In addition to CNNs and ViTs, there are hybrid approaches
            that combine the two. One particular example is HiMODE, an
            omnidirectional monocular depth estimation model. While the sensor
            is different due to being monocular, the concept for the model
            architecure still applies. HiMODE models the Transformer
            architecture but incorporates a convolution feature extractor and a
            Spatial Residual Block (SRB) which includes convolutions.
        </p>
    </div>
</body>

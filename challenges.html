<!DOCTYPE html>
<head>
    <title>CS663 Research Tutorial</title>
    <link rel="stylesheet" href="index.css" />
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {},
            },
        };
    </script>
</head>
<body class="dark:bg-slate-800">
    <nav>
        <ul
            class="text-2xl flex flex-row justify-evenly slate-300 dark:slate-500 text-black dark:text-white"
        >
            <li>
                <a href="/"><div class="p-4">Abstract</div></a>
            </li>
            <li>
                <a href="/introduction"><div class="p-4">Introduction</div></a>
            </li>
            <li>
                <a href="/sensors"><div class="p-4">Sensors</div></a>
            </li>
            <li>
                <a href="/models"><div class="p-4">Models</div></a>
            </li>
            <li>
                <a href="/tradeoffs"><div class="p-4">Tradeoffs</div></a>
            </li>
            <li>
                <a href="/challenges">
                    <div class="p-4">Challenges</div>
                </a>
            </li>
            <li>
                <a href="/conclusions"><div class="p-4">Conclusions</div> </a>
            </li>
        </ul>
    </nav>
    <div class="p-8 w-full min-h-screen">
        <h1 class="mb-4 text-4xl text-center font-semibold dark:text-white">
            Challenges
        </h1>
        <p class="text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; Sticking with a monocular camera, the most significant
            challenge is that of choosing the correct model. In general, a
            monocular camera with a CNN will work for low-end systems. The model
            could be upgraded if the cost is applicable to a Vision Transformer
            if the device has the computing power available. For high-end
            hardware, running a complex model and having a LiDAR sensor is the
            ideal combination. Alternatively, a high-end model could be deployed
            on the cloud and the mobile device could request from it. The main
            challenge then would be handling the latency and requiring an
            internet connection at all times. Generally speaking, there is also
            the challenge of developing the software around the model itself.
            With the example of individuals with differing visual impairments,
            the challenge would be communicating depth information without
            visuals.
        </p>
        <h2 class="p-2 text-3xl text-center dark:text-slate-200">
            Application Development
        </h2>
        <div class="mt-2 text-lg font-light w-1/2 mx-auto dark:text-white">
            <figure>
                <img
                    class="p-2 max-w-1/2 w-3/4 h-auto rounded-xl bg-slate-400"
                    src="assets/clouddiagram.png"
                    alt="Diagram Comparing Local and Cloud Model Application Architectures"
                />
            </figure>
            <figcaption class="">
                <p class="mx-auto p-1 font-semibold text-center">
                    Diagram Comparing Local and Cloud-Based Application
                    Architectures
                </p>
            </figcaption>
        </div>

        <p class="text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; Another major challenge is developing an application around
            depth estimation.
        </p>
        <h2 class="p-2 text-3xl text-center dark:text-slate-200">
            User Experience
        </h2>
        <p class="text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; Having a good user experience is yet another challenge to
            tackle for creating a worthwhile application for various use cases
            and especially accessibility. Another major factor in the quality of
            the depth outputs is whether the sensor is extrinsically calibrated
            for the application. For accessibility in particular, the entire
            system needs to be designed in a way that accounts for visual
            impairment. In the diagram depicted, the user is wearing the device,
            removing the need to physically hold the camera to identify depth
            and instead automatically detecting immediately in front of the
            user, which is ideal for use cases like obstacle detection.
            Additionally, while the diagram uses a stereo camera, the same
            method applies to other sensors as well, including monocular
            cameras.
        </p>

        <div class="mt-2 text-lg font-light w-1/2 mx-auto dark:text-white">
            <figure>
                <img
                    class="p-2 max-w-1/2 w-3/4 h-auto rounded-xl bg-slate-400"
                    src="assets/userdiagram.gif"
                    alt="Diagram of a Visually-Impaired Person Utilizing the Application"
                />
            </figure>
            <figcaption class="">
                <a href="https://ieeexplore.ieee.org/document/6820289">
                    <p class="mx-auto p-1 font-semibold text-center">
                        Diagram of a Visually-Impaired Person Utilizing the
                        Application
                    </p>
                </a>
            </figcaption>
        </div>
    </div>
</body>

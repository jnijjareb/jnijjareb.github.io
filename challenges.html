<!DOCTYPE html>
<head>
    <title>CS663 Research Tutorial</title>
    <link rel="stylesheet" href="index.css" />
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {},
            },
        };
    </script>
</head>
<body class="dark:bg-slate-800">
    <nav>
        <ul
            class="text-2xl flex flex-row justify-evenly slate-300 dark:slate-500 text-black dark:text-white"
        >
            <li>
                <a href="/"><div class="p-4">Abstract</div></a>
            </li>
            <li>
                <a href="/introduction"><div class="p-4">Introduction</div></a>
            </li>
            <li>
                <a href="/sensors"><div class="p-4">Sensors</div></a>
            </li>
            <li>
                <a href="/models"><div class="p-4">Models</div></a>
            </li>
            <li>
                <a href="/tradeoffs"><div class="p-4">Tradeoffs</div></a>
            </li>
            <li>
                <a href="/challenges">
                    <div class="p-4">Challenges</div>
                </a>
            </li>
            <li>
                <a href="/conclusions"><div class="p-4">Conclusions</div> </a>
            </li>
        </ul>
    </nav>
    <div class="p-8 w-full min-h-screen">
        <h1 class="mb-4 text-4xl text-center font-semibold dark:text-white">
            Challenges
        </h1>
        <p class="text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; Sticking with a monocular camera, the most significant
            challenge is that of choosing the correct model. In general, a
            monocular camera with a CNN will work for low-end systems. The model
            could be upgraded if the cost is applicable to a Vision Transformer
            if the device has the computing power available. For high-end
            hardware, running a complex model and having a LiDAR sensor is the
            ideal combination. Alternatively, a high-end model could be deployed
            on the cloud and the mobile device could request from it. The main
            challenge then would be handling the latency and requiring an
            internet connection at all times. Generally speaking, there is also
            the challenge of developing the software around the model itself.
            With the example of individuals with differing visual impairments,
            the challenge would be communicating depth information without
            visuals.
        </p>
        <h2 class="p-2 text-3xl text-center dark:text-slate-200">
            Application Development
        </h2>
        <p class="text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; Another major challenge is developing an application around
            depth estimation. An application for accessibility requires the
            ability to communicate information acquired from depth sensing to
            the user. Furthermore, building around a machine learning model,
            which is computationally expensive, also requires preprocessing
            sensor data for input, postprocessing model outputs, and acquiring
            insights from the postprocessed outputs to communciate to the user,
            all of which further increase the computational load on the device.
            Additionally, since some applications may opt to use a client-server
            architecture and run the model on a dedicated cloud server with far
            more power than a mobile device, network issues and latency has to
            be considered. If a user needs to venture into areas without a
            strong internet connection as would be required for sending a live
            feed of images, various network failure cases need to be considered
            and factored into the user experience. From a development
            standpoint, data sent over the network and server infrastructure are
            more factors to optimize.
        </p>
        <div class="mt-2 text-lg font-light w-1/2 mx-auto dark:text-white">
            <figure>
                <img
                    class="p-2 max-w-1/2 w-3/4 h-auto rounded-xl bg-slate-400"
                    src="assets/clouddiagram.png"
                    alt="Diagram Comparing Local and Cloud Model Application Architectures"
                />
            </figure>
            <figcaption class="">
                <p class="mx-auto p-1 font-semibold text-center">
                    Diagram Comparing Local and Cloud-Based Application
                    Architectures
                </p>
            </figcaption>
        </div>

        <h2 class="p-2 text-3xl text-center dark:text-slate-200">
            User Experience
        </h2>
        <p class="text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; Having a good user experience is yet another challenge to
            tackle for creating a worthwhile application for various use cases
            and especially accessibility. Another major factor in the quality of
            the depth outputs is whether the sensor is extrinsically calibrated
            for the application. For accessibility in particular, the entire
            system needs to be designed in a way that accounts for visual
            impairment [<a class="text-blue-400" href="introduction/#ref1">1</a
            >]. In the diagram depicted, the user is wearing the device,
            removing the need to physically hold the camera to identify depth
            and instead automatically detecting immediately in front of the
            user, which is ideal for use cases like obstacle detection.
            Additionally, while the diagram uses a stereo camera, the same
            method applies to other sensors as well, including monocular
            cameras.
        </p>

        <div class="mt-2 text-lg font-light w-1/2 mx-auto dark:text-white">
            <figure>
                <img
                    class="p-2 max-w-1/2 w-3/4 h-auto rounded-xl bg-slate-400"
                    src="assets/userdiagram.gif"
                    alt="Diagram of a Visually-Impaired Person Utilizing the Application"
                />
            </figure>
            <figcaption class="">
                <a href="https://ieeexplore.ieee.org/document/6820289">
                    <p class="mx-auto p-1 font-semibold text-center">
                        Diagram of a Visually-Impaired Person Utilizing the
                        Application
                    </p>
                </a>
            </figcaption>
        </div>
        <h2 class="p-2 text-3xl text-center dark:text-slate-200">
            Communication of Depth Information
        </h2>
        <p class="text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; Different visual impairments and use cases prompt different
            methods of communicating the depth information. With the example of
            frontal obstacle detection, an auditory cue could be one option. As
            part of designing the application, utilizing such an audio interface
            requires crafting an auditory signal to indicate danger. There is
            also the challenge of how having an audio device would affect the
            surrounding environment. Nearby people may find auditory indications
            to be abrasive or an annoyance, not to mention how the user will
            experience the signal repetitively. For individuals with low vision
            or a lack of depth perception capabilities who could still view a
            smartphone screen, visual information could be a better option. The
            depth sensing feature could be used alongside a standard magnifier
            as some accessibility applications already do to not only convey
            their surroundings, but add depth perception to them as well.
        </p>
    </div>
</body>

<!doctype html>
<head>
    <title>CS663 Research Tutorial</title>
    <link rel="stylesheet" href="index.css" />
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {},
            },
        };
    </script>
</head>
<body class="dark:bg-slate-800">
    <nav>
        <ul
            class="text-2xl flex flex-row justify-evenly slate-300 dark:slate-500 text-black dark:text-white"
        >
            <li>
                <a href="/"><div class="p-4">Abstract</div></a>
            </li>
            <li>
                <a href="/introduction"><div class="p-4">Introduction</div></a>
            </li>
            <li>
                <a href="/sensors"><div class="p-4">Sensors</div></a>
            </li>
            <li>
                <a href="/models"><div class="p-4">Models</div></a>
            </li>
            <li>
                <a href="/tradeoffs"><div class="p-4">Tradeoffs</div></a>
            </li>
            <li>
                <a href="/challenges">
                    <div class="p-4">Challenges</div>
                </a>
            </li>
            <li>
                <a href="/conclusions"><div class="p-4">Conclusions</div> </a>
            </li>
        </ul>
    </nav>
    <div class="p-8 w-full min-h-screen">
        <h1 class="mb-4 text-4xl text-center font-semibold dark:text-white">
            Depth Estimation for Visually-Impaired Individuals
        </h1>
        <h2 class="p-2 text-3xl text-center dark:text-slate-200">Abstract</h2>
        <p class="text-lg font-light w-1/2 mx-auto dark:text-white">
            &emsp; Of the various challenges faced by vision-impaired
            individuals, one particular problem is obtaining information to
            emulate depth perception. In addition to having an understanding of
            2-dimensional visual information presented, there exists a problem
            of understanding depth and the 3-dimensional nature of surroundings.
            Depth estimation models are capable of estimating how far objects
            are from the viewer. Convolutional Neural Networks (CNNs), Vision
            Transformers, and approaches derived from the two are some of the
            modern deep learning model architectures for depth estimation.
            Mobile devices are an ideal candidate for a widely available device
            to provide depth estimation with its sensors. Several sensor types
            have been tested for the task with tradeoffs on performance, cost,
            convenience, and compatibility with mobile phones for the user. The
            environment also significantly impacts the performance of the
            various available options such as an indoor scene as opposed to an
            outdoor one. Additionally, environmental lighting is a variable to
            consider given that specific sensors can be advantageous in specific
            lighting conditions. The project examines the different approaches
            to depth estimation, their strengths and weaknesses, and the impact
            on accessibility.
        </p>
        <div class="p-4 flex-col w-1/2 mx-auto">
            <div class="m-2 p-2 bg-slate-200 rounded-xl">
                <a
                    class="text-2xl text-blue-600 font-semibold"
                    href="introduction"
                    >Introduction</a
                >
            </div>
            <div class="m-2 p-2 bg-slate-200 rounded-xl">
                <a class="text-2xl text-blue-600 font-semibold" href="sensors"
                    >Sensors</a
                >
            </div>
            <div class="m-2 p-2 bg-slate-200 rounded-xl">
                <a class="text-2xl text-blue-600 font-semibold" href="models"
                    >Depth Estimation Models</a
                >
            </div>
            <div class="m-2 p-2 bg-slate-200 rounded-xl">
                <a class="text-2xl text-blue-600 font-semibold" href="tradeoffs"
                    >Effectiveness and Tradeoffs</a
                >
            </div>
            <div class="m-2 p-2 bg-slate-200 rounded-xl">
                <a
                    class="text-2xl text-blue-600 font-semibold"
                    href="challenges"
                    >Challenges</a
                >
            </div>
            <div class="m-2 p-2 bg-slate-200 rounded-xl">
                <a
                    class="text-2xl text-blue-600 font-semibold"
                    href="conclusions"
                    >Conclusions and Future</a
                >
            </div>
        </div>
        <div class="mx-auto flex-col w-1/2 font-light dark:text-white">
            <p class="">
                [1] F. Navarro, S. Cancino, and E. Estupinan, &apos;Depth
                estimation for visually impaired people using mobile
                devices&apos;, in 2014 IEEE 5th Latin American Symposium on
                Circuits and Systems, Feb. 2014, pp. 1&dash;4. doi:
                10.1109/LASCAS.2014.6820289.
            </p>
            <br />
            <p>
                Synopsis: The paper covers the process of creating an
                application that can utilize mobile devices to perform depth
                estimation. It discusses different environemnts and the
                mechanics of a visually-impaired person using such a system.
            </p>
            <p>
                Reliability: IEEE is a well-known and established organization
                focused on engineering.
            </p>

            <br />
            <p class="">
                [2] K. Park, S. Kim, and K. Sohn, &apos;High-Precision Depth
                Estimation Using Uncalibrated LiDAR and Stereo Fusion&apos;,
                IEEE Trans. Intell. Transp. Syst., vol. 21, no. 1, pp.
                321&dash;335, Jan. 2020, doi: 10.1109/TITS.2019.2891788.
            </p>
            <p>
                Synopsis: The paper covers the usage of a combination of LiDAR
                and Stereo Vision for 3D reconstruction. Notably, it accumulates
                input from multiple types of sensors and utilizes a
                Convolutional Neural Network to perform reconstruction.
            </p>
            <p>
                Reliability: IEEE is a well-known and established organization
                focused on engineering.
            </p>

            <br />
            <p class="">
                [3] M. S. Junayed, A. Sadeghzadeh, M. B. Islam, L.-K. Wong, and
                T. Aydin, &apos;HiMODE: A Hybrid Monocular Omnidirectional Depth
                Estimation Model&apos;, Apr. 11, 2022, arXiv: arXiv:2204.05007.
                Available:
                <a
                    class="text-blue-400"
                    href="http://arxiv.org/abs/2204.05007"
                    target="_blank"
                    >http://arxiv.org/abs/2204.05007</a
                >
            </p>

            <br />
            <p>
                Synopsis: The paper introduces a Convolutional Neural Network
                and Vision Transformer hybrid architecture called HiMODE. The
                method is intended for omnidirectional monocular depth
                estimation.
            </p>
            <p>
                Reliability: Multiple researchers are from private institutions
                in Turkey, with one from a private institution in Malaysia and
                another from the American University of Malta.
            </p>

            <br />
            <p class="">
                [4] L. Papa, P. Russo, and I. Amerini, &apos;METER: a mobile
                vision transformer architecture for monocular depth
                estimation&apos;. Available:
                <a
                    class="text-blue-400"
                    href="https://arxiv.org/html/2403.08368v1"
                    target="_blank"
                >
                    https://arxiv.org/html/2403.08368v1
                </a>
            </p>

            <br />
            <p>
                Synopsis: The paper introduces a lightweight Vision Transformer
                architecture designed for mobile devices called METER. The
                architecture includes blocks from Convolutional Neural Networks
                and Transformers alongside its own custom METER block.
            </p>
            <p>
                Reliability: The authors are from Sapienza University of Rome in
                Italy.
            </p>

            <br />
            <p class="">
                [5] A. Ganj, H. Su, and T. Guo, &apos;HybridDepth: Robust Depth
                Fusion for Mobile AR by Leveraging Depth from Focus and
                Single&dash;Image Priors&apos;. Available:

                <a
                    class="text-blue-400"
                    href="https://arxiv.org/pdf/2407.18443v1"
                    target="_blank"
                >
                    https://arxiv.org/pdf/2407.18443v1</a
                >
            </p>

            <br />
            <p>
                Synopsis: The paper covers a model architecture called
                HybridDepth, combining standard relative depth estimation with
                Depth From Focus. The method is intended for Augmented Reality
                from mobile devices.
            </p>
            <p>
                Reliability: One author is from NVIDIA Corporation and the other
                two are from Worcester Polytechnic Institute in Massachusetts.
            </p>
        </div>
    </div>
</body>
